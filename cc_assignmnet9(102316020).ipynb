{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86ea7c2b-255c-4476-a646-1de495d4b67c",
   "metadata": {},
   "source": [
    "Q1. Write a unique paragraph (5-6 sentences) about your favorite topic (e.g., sports,\n",
    "technology, food, books, etc.).\n",
    "1. Convert text to lowercase and remove punctuation.\n",
    "2. Tokenize the text into words and sentences.\n",
    "3. Remove stopwords (using NLTK's stopwords list).\n",
    "4. Display word frequency distribution (excluding stopwords)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7bdd5a8-dcd8-4187-aa15-0df4cd9f8a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Swast\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Swast\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Swast\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Swast\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')       # For tokenization\n",
    "nltk.download('stopwords')   # For stopword removal\n",
    "nltk.download('wordnet')     # For lemmatization\n",
    "nltk.download('punkt_tab')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bed7583-65c1-41cf-a9a8-c202a36894ed",
   "metadata": {},
   "source": [
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import string\n",
    "\n",
    "text = \"\"\"\n",
    "In recent years, artificial intelligence has revolutionized the way we interact with technology.\n",
    "From voice assistants responding to casual conversation to self‑driving cars navigating busy streets, the impact is profound.\n",
    "Developers are racing to build smarter algorithms that can learn and adapt in real time.\n",
    "Meanwhile, concerns around data privacy and ethical use of AI continue to grow.\n",
    "It is clear that the next decade will be shaped by breakthroughs in machine learning and neural network research.\n",
    "\"\"\"\n",
    "\n",
    "# 2. Lowercase & remove punctuation\n",
    "clean = text.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "# 3. Tokenize\n",
    "sentences = sent_tokenize(clean)\n",
    "words = word_tokenize(clean)\n",
    "\n",
    "# 4. Remove stopwords\n",
    "stops = set(stopwords.words('english'))\n",
    "filtered = [w for w in words if w not in stops and w.isalpha()]\n",
    "\n",
    "# 5. Word frequency\n",
    "freq = Counter(filtered)\n",
    "print(\"Sentence tokens:\", sentences)\n",
    "print(\"Word tokens:\", words)\n",
    "print(\"Filtered words:\", filtered)\n",
    "print(\"Word Frequency Distribution:\")\n",
    "for word, count in freq.most_common():\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e429065-21f4-49fd-99e5-e378ed829c13",
   "metadata": {},
   "source": [
    "Q2: Stemming and Lemmanization\n",
    "1. Take the tokenized words from Question 1 (aŌer stopword removal).\n",
    "2. Apply stemming using NLTK's PorterStemmer and LancasterStemmer.\n",
    "3. Apply lemmanization using NLTK's WordNetLemmaƟzer.\n",
    "4. Compare and display results of both techniques. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9218f8d2-ec41-4176-be8f-55b4b459aaf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          original        porter     lancaster           lemma\n",
      "0           recent        recent           rec          recent\n",
      "1            years          year          year            year\n",
      "2       artificial      artifici           art      artificial\n",
      "3     intelligence      intellig      intellig    intelligence\n",
      "4   revolutionized    revolution        revolv  revolutionized\n",
      "5              way           way           way             way\n",
      "6         interact      interact      interact        interact\n",
      "7       technology     technolog     technolog      technology\n",
      "8            voice          voic          voic           voice\n",
      "9       assistants        assist        assist       assistant\n",
      "10      responding       respond       respond      responding\n",
      "11          casual        casual           cas          casual\n",
      "12    conversation       convers       convers    conversation\n",
      "13            cars           car           car             car\n",
      "14      navigating         navig         navig      navigating\n",
      "15            busy          busi          busy            busy\n",
      "16         streets        street       streets          street\n",
      "17          impact        impact        impact          impact\n",
      "18        profound      profound      profound        profound\n",
      "19      developers       develop       develop       developer\n",
      "20          racing          race           rac          racing\n",
      "21           build         build         build           build\n",
      "22         smarter       smarter         smart         smarter\n",
      "23      algorithms     algorithm     algorithm       algorithm\n",
      "24           learn         learn         learn           learn\n",
      "25           adapt         adapt         adapt           adapt\n",
      "26            real          real          real            real\n",
      "27            time          time           tim            time\n",
      "28       meanwhile      meanwhil      meanwhil       meanwhile\n",
      "29        concerns       concern       concern         concern\n",
      "30          around        around        around          around\n",
      "31            data          data           dat            data\n",
      "32         privacy       privaci          priv         privacy\n",
      "33         ethical         ethic           eth         ethical\n",
      "34             use           use            us             use\n",
      "35              ai            ai            ai              ai\n",
      "36        continue       continu       continu        continue\n",
      "37            grow          grow          grow            grow\n",
      "38           clear         clear         clear           clear\n",
      "39            next          next          next            next\n",
      "40          decade         decad         decad          decade\n",
      "41          shaped         shape          shap          shaped\n",
      "42   breakthroughs  breakthrough  breakthrough    breakthrough\n",
      "43         machine        machin        machin         machine\n",
      "44        learning         learn         learn        learning\n",
      "45          neural        neural          neur          neural\n",
      "46         network       network       network         network\n",
      "47        research      research      research        research\n"
     ]
    }
   ],
   "source": [
    "# Q2: stemming & lemmatization\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer\n",
    "\n",
    "# If running first time, you may need:\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "results = []\n",
    "for w in filtered:\n",
    "    results.append({\n",
    "        'original': w,\n",
    "        'porter': porter.stem(w),\n",
    "        'lancaster': lancaster.stem(w),\n",
    "        'lemma': lemmatizer.lemmatize(w)\n",
    "    })\n",
    "\n",
    "# Display in a neat table\n",
    "import pandas as pd\n",
    "df2 = pd.DataFrame(results)\n",
    "print(df2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f189b24-fdac-4625-be5e-ed49f73b58bb",
   "metadata": {},
   "source": [
    "Q3. Regular Expressions and Text Spliƫng\n",
    "1. Take their original text from Question 1.\n",
    "2. Use regular expressions to:\n",
    "a. Extract all words with more than 5 letters.\n",
    "b. Extract all numbers (if any exist in their text).\n",
    "c. Extract all capitalized words.\n",
    "3. Use text spliƫng techniques to:\n",
    "a. Split the text into words containing only alphabets (removing digits and special\n",
    "characters).\n",
    "b. Extract words starƟng with a vowel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b5653029-3661-4dc2-94a5-aedcaf6c17bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words >5 letters: ['recent', 'artificial', 'intelligence', 'revolutionized', 'interact', 'technology', 'assistants', 'responding', 'casual', 'conversation', 'driving', 'navigating', 'streets', 'impact', 'profound', 'Developers', 'racing', 'smarter', 'algorithms', 'Meanwhile', 'concerns', 'around', 'privacy', 'ethical', 'continue', 'decade', 'shaped', 'breakthroughs', 'machine', 'learning', 'neural', 'network', 'research']\n",
      "Numbers: []\n",
      "Capitalized words: ['In', 'From', 'Developers', 'Meanwhile', 'It']\n",
      "Alpha-only words: ['In', 'recent', 'years', 'artificial', 'intelligence', 'has', 'revolutionized', 'the', 'way', 'we', 'interact', 'with', 'technology', 'From', 'voice', 'assistants', 'responding', 'to', 'casual', 'conversation', 'to', 'self', 'driving', 'cars', 'navigating', 'busy', 'streets', 'the', 'impact', 'is', 'profound', 'Developers', 'are', 'racing', 'to', 'build', 'smarter', 'algorithms', 'that', 'can', 'learn', 'and', 'adapt', 'in', 'real', 'time', 'Meanwhile', 'concerns', 'around', 'data', 'privacy', 'and', 'ethical', 'use', 'of', 'AI', 'continue', 'to', 'grow', 'It', 'is', 'clear', 'that', 'the', 'next', 'decade', 'will', 'be', 'shaped', 'by', 'breakthroughs', 'in', 'machine', 'learning', 'and', 'neural', 'network', 'research']\n",
      "Words starting with vowel: ['In', 'artificial', 'intelligence', 'interact', 'assistants', 'impact', 'is', 'are', 'algorithms', 'and', 'adapt', 'in', 'around', 'and', 'ethical', 'use', 'of', 'AI', 'It', 'is', 'in', 'and']\n"
     ]
    }
   ],
   "source": [
    "# Q3: regex & splitting\n",
    "import re\n",
    "\n",
    "orig = text  # use the original (with punctuation & case)\n",
    "\n",
    "# a. >5 letters\n",
    "long_words = re.findall(r'\\b\\w{6,}\\b', orig)\n",
    "# b. Numbers\n",
    "numbers = re.findall(r'\\d+(?:\\.\\d+)?', orig)\n",
    "# c. Capitalized words\n",
    "capitalized = re.findall(r'\\b[A-Z][a-z]+\\b', orig)\n",
    "\n",
    "# d. Split into alpha-only words\n",
    "alpha_words = re.findall(r'\\b[A-Za-z]+\\b', orig)\n",
    "# e. Words starting with vowel\n",
    "vowel_words = [w for w in alpha_words if re.match(r'^[AEIOUaeiou]', w)]\n",
    "\n",
    "print(\"Words >5 letters:\", long_words)\n",
    "print(\"Numbers:\", numbers)\n",
    "print(\"Capitalized words:\", capitalized)\n",
    "print(\"Alpha-only words:\", alpha_words)\n",
    "print(\"Words starting with vowel:\", vowel_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa071100-933c-455f-b555-c339e372483a",
   "metadata": {},
   "source": [
    "Q4. Custom TokenizaƟon & Regex-based Text Cleaning\n",
    "1. Take original text from QuesƟon 1.\n",
    "2. Write a custom tokenizaƟon funcƟon that:\n",
    "a. Removes punctuaƟon and special symbols, but keeps contracƟons (e.g.,\n",
    "\"isn't\" should not be split into \"is\" and \"n't\").\n",
    "b. Handles hyphenated words as a single token (e.g., \"state-of-the-art\" remains\n",
    "a single token).\n",
    "c. Tokenizes numbers separately but keeps decimal numbers intact (e.g., \"3.14\"\n",
    "should remain as is).\n",
    "3. Use Regex SubsƟtuƟons (re.sub) to:\n",
    "a. Replace email addresses with '<EMAIL>' placeholder.\n",
    "b. Replace URLs with '<URL>' placeholder.\n",
    "c. Replace phone numbers (formats: 123-456-7890 or +91 9876543210) with\n",
    "'<PHONE>' placeholder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "63a42110-20a7-4552-b1a5-35fa285278b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom tokens: ['In', 'recent', 'years', 'artificial', 'intelligence', 'has', 'revolutionized', 'the', 'way', 'we', 'interact', 'with', 'technology', 'From', 'voice', 'assistants', 'responding', 'to', 'casual', 'conversation', 'to', 'self', 'driving', 'cars', 'navigating', 'busy', 'streets', 'the', 'impact', 'is', 'profound', 'Developers', 'are', 'racing', 'to', 'build', 'smarter', 'algorithms', 'that', 'can', 'learn', 'and', 'adapt', 'in', 'real', 'time', 'Meanwhile', 'concerns', 'around', 'data', 'privacy', 'and', 'ethical', 'use', 'of', 'AI', 'continue', 'to', 'grow', 'It', 'is', 'clear', 'that', 'the', 'next', 'decade', 'will', 'be', 'shaped', 'by', 'breakthroughs', 'in', 'machine', 'learning', 'and', 'neural', 'network', 'research']\n"
     ]
    }
   ],
   "source": [
    "# Q4: custom tokenizer & cleaning\n",
    "def custom_tokenize(s):\n",
    "    # placeholder for emails/URLs/phones before tokenization\n",
    "    s = re.sub(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b', '<EMAIL>', s)\n",
    "    s = re.sub(r'https?://\\S+|www\\.\\S+', '<URL>', s)\n",
    "    s = re.sub(r'\\+?\\d{1,3}[\\s-]\\d{6,10}', '<PHONE>', s)\n",
    "    s = re.sub(r'\\b\\d+\\.\\d+\\b', lambda m: f\"<NUM:{m.group(0)}>\", s)  # mark decimals\n",
    "    s = re.sub(r'\\b\\d+\\b', '<NUM>', s)\n",
    "    # now split on whitespace & punctuation except hyphens/apostrophes\n",
    "    tokens = re.findall(r\"[A-Za-z0-9<>:_]+(?:['-][A-Za-z0-9]+)*\", s)\n",
    "    # restore decimal tokens\n",
    "    return [t.replace('<NUM:', '').replace('>', '') if t.startswith('<NUM:') else t for t in tokens]\n",
    "\n",
    "sample = text.strip()\n",
    "tokens_q4 = custom_tokenize(sample)\n",
    "print(\"Custom tokens:\", tokens_q4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3bf847-7c6e-4cc6-be68-823b4dfe5090",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
